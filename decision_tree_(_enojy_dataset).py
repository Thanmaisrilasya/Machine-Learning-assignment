# -*- coding: utf-8 -*-
"""Decision Tree ( enojy dataset).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GPjuNBW6wc7kMZoqZRizH7SrXnfKtNj5
"""

import pandas as pd


data = {
    'Sky': ['Sunny', 'Sunny', 'Rainy', 'Sunny'],
    'AirTemp': ['Warm', 'Warm', 'Cold', 'Warm'],
    'Humidity': ['Normal', 'High', 'High', 'High'],
    'Wind': ['Strong', 'Strong', 'Strong', 'Strong'],
    'Water': ['Warm', 'Warm', 'Warm', 'Cool'],
    'Forecast': ['Same', 'Same', 'Change', 'Change'],
    'EnjoySport': ['Yes', 'Yes', 'No', 'Yes']
}

df = pd.DataFrame(data)
print(df)

import numpy as np

def entropy(target_column):
    elements, counts = np.unique(target_column, return_counts=True)
    entropy_value = 0
    for i in range(len(elements)):
        probability = counts[i] / np.sum(counts)
        entropy_value -= probability * np.log2(probability)
    return entropy_value

def information_gain(data, split_attribute, target_attribute="EnjoySport"):
    total_entropy = entropy(data[target_attribute])
    vals, counts = np.unique(data[split_attribute], return_counts=True)

    weighted_entropy = 0
    for i in range(len(vals)):
        subset = data[data[split_attribute] == vals[i]]
        prob = counts[i] / np.sum(counts)
        weighted_entropy += prob * entropy(subset[target_attribute])

    info_gain = total_entropy - weighted_entropy
    return info_gain

class DecisionTree:

    def __init__(self, df, target_attribute="EnjoySport"):
        self.df = df
        self.target_attribute = target_attribute
        self.tree = self.build_tree(df)

    def build_tree(self, data, tree=None):
        target_values = np.unique(data[self.target_attribute])

        # Base cases: If all target values are the same, return this as the classification
        if len(target_values) == 1:
            return target_values[0]

        # If no more attributes are left to split, return the majority class
        elif len(data.columns) == 1:
            return data[self.target_attribute].mode()[0]

        # Recursive case: Choose the best attribute to split on
        else:
            info_gains = {}
            for attribute in data.columns:
                if attribute != self.target_attribute:
                    info_gains[attribute] = information_gain(data, attribute, self.target_attribute)

            # Best attribute is the one with the highest information gain
            best_attribute = max(info_gains, key=info_gains.get)

            # Create the tree root with the best attribute
            if tree is None:
                tree = {}
                tree[best_attribute] = {}

            # Split the dataset by the best attribute
            attribute_values = np.unique(data[best_attribute])

            for value in attribute_values:
                subset = data[data[best_attribute] == value].drop([best_attribute], axis=1)

                # Recursively build the tree for each branch
                subtree = self.build_tree(subset)
                tree[best_attribute][value] = subtree

        return tree

    def print_tree(self, tree=None, indent=""):
        if tree is None:
            tree = self.tree

        if isinstance(tree, dict):
            for attribute in tree:
                for value in tree[attribute]:
                    print(indent, attribute, "=", value, ":")
                    self.print_tree(tree[attribute][value], indent + "    ")
        else:
            print(indent, tree)

# Building the tree using the dataset
dt = DecisionTree(df)
dt.print_tree()

import pandas as pd


data = {
    'Sky': ['Sunny', 'Sunny', 'Rainy', 'Sunny', 'Sunny', 'Rainy', 'Rainy', 'Rainy'],
    'AirTemp': ['Warm', 'Warm', 'Cold', 'Warm', 'Warm', 'Cold', 'Cold', 'Warm'],
    'Humidity': ['Normal', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High'],
    'Wind': ['Strong', 'Strong', 'Strong', 'Strong', 'Weak', 'Weak', 'Strong', 'Weak'],
    'Water': ['Warm', 'Warm', 'Warm', 'Warm', 'Cool', 'Cool', 'Warm', 'Cool'],
    'Forecast': ['Same', 'Same', 'Change', 'Same', 'Same', 'Same', 'Change', 'Change'],
    'EnjoySport': ['Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes']
}


df = pd.DataFrame(data)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

for col in df.columns:
    df[col] = le.fit_transform(df[col])

print(df)

from sklearn.tree import DecisionTreeClassifier

X = df.drop(columns=['EnjoySport'])
y = df['EnjoySport']

clf = DecisionTreeClassifier()
clf = clf.fit(X, y)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(12,8))
plot_tree(clf, feature_names=X.columns, class_names=['0', '1'], filled=True)
plt.show()