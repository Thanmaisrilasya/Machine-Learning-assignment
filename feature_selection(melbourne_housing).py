# -*- coding: utf-8 -*-
"""Feature_selection(Melbourne_housing).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12UHoLZNJsreErYdoYXp0LjRlI6RpY24x
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_selection import VarianceThreshold, SequentialFeatureSelector
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv(r"melbourne_housing_raw.csv")

# Display basic information about the dataset
print("Initial dataset shape:", data.shape)
print("Missing values per column:\n", data.isnull().sum())

# Remove columns with more than 20% missing values
missing_limit = 20
missing_percentage = data.isnull().mean() * 100
filtered_data = data.loc[:, missing_percentage <= missing_limit]

# Display the shape of the data after filtering
print("\nShape after removing columns with more than 20% missing values:", filtered_data.shape)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
housing_data = pd.read_csv("melbourne_housing_raw.csv")

# Drop rows with missing values to clean the dataset
cleaned_data = housing_data.dropna()

# Select only numeric columns for correlation analysis
numeric_data = cleaned_data.select_dtypes(include=[np.number])

# Calculate the correlation matrix for numeric columns
corr_matrix = numeric_data.corr()

# Identify features with high correlation (correlation > 0.85)
corr_threshold = 0.85
high_corr_indices = np.where(corr_matrix > corr_threshold)

# Find pairs of highly correlated features
high_correlation_pairs = [(corr_matrix.index[x], corr_matrix.columns[y])
                          for x, y in zip(*high_corr_indices) if x != y and x < y]

print("Highly correlated features (correlation > 0.85):\n", high_correlation_pairs)

# Create a set of features to remove based on high correlation
features_to_drop = set([pair[1] for pair in high_correlation_pairs])
reduced_data = cleaned_data.drop(columns=features_to_drop)

# Convert categorical variables into numerical values using OneHotEncoder
reduced_data = pd.get_dummies(reduced_data, drop_first=True)

# Define the features (X) and target variable (y)
X = reduced_data.drop(columns=['Price'])
y = reduced_data['Price']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Make predictions and evaluate the model
predictions = linear_model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
print(f"Mean Squared Error: {mse}")

r_squared = r2_score(y_test, predictions)
print(f"R-squared: {r_squared}")

cleaned_dataframe = data.dropna()
# OR
cleaned_dataframe = housing_data.dropna()

# Convert categorical variables into numerical values using OneHotEncoder or get_dummies
encoded_dataframe = pd.get_dummies(cleaned_dataframe, drop_first=True)

# Define features (X) and target variable (y)
X = encoded_dataframe.drop(columns=['Price'])
y = encoded_dataframe['Price']

# Calculate the variance of all features
feature_variance = X.var()
# print("Variance of features before filtering:")
# print(feature_variance)

# Set a threshold to filter out features with low variance
variance_threshold_value = 0.01
variance_filter = VarianceThreshold(threshold=variance_threshold_value)
X_high_variance = variance_filter.fit_transform(X)

# Identify indices of features that were retained and those that were removed
retained_features = X.columns[variance_filter.get_support()]
removed_features = X.columns[~variance_filter.get_support()]

print("\nFeatures removed after applying variance threshold:")
print(removed_features)

from sklearn.feature_selection import SequentialFeatureSelector

# Initialize Sequential Feature Selector for forward selection
# Reduce number of folds for cross-validation (from 5 to 3) and limit features to select
sfs = SequentialFeatureSelector(model, n_features_to_select=10, direction="forward", cv=3)

# Fit the feature selector to the training data
sfs.fit(X_train, y_train)

# Get the selected features
selected_features = sfs.get_support(indices=True)

# Retrieve the feature names for the selected features
selected_feature_names = X.columns[selected_features]
print("Selected feature names:", selected_feature_names)

# Use only selected features for training and testing
X_train_selected = X_train.iloc[:, selected_features]
X_test_selected = X_test.iloc[:, selected_features]

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv("melbourne_housing_raw.csv")

# Handle missing values by dropping rows with missing values
df_cleaned = df.dropna()

# Convert categorical variables into numerical values using get_dummies
df_encoded = pd.get_dummies(df_cleaned, drop_first=True)

# Define features (X) and target (y)
X = df_encoded.drop(columns=['Price'])
y = df_encoded['Price']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Function to perform backward feature elimination
def backward_feature_elimination(X_train, X_test, y_train, y_test, model):
    features = X_train.columns.tolist()  # List of feature names
    scores = []  # List to store (number of features, R-squared score)

    while len(features) > 0:
        # Train the model with current features
        model.fit(X_train[features], y_train)
        # Make predictions
        y_pred = model.predict(X_test[features])
        # Calculate and store the score (R-squared)
        r2 = r2_score(y_test, y_pred)
        scores.append((len(features), r2))

        # Get feature importances
        importances = model.feature_importances_
        # Create a DataFrame for feature importance
        importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
        # Remove the least important feature
        least_important_feature = importance_df.nsmallest(1, 'Importance')['Feature'].values[0]
        features.remove(least_important_feature)

        print(f"Removed feature: {least_important_feature} | Remaining features: {len(features)}")

    return scores

# Run backward feature elimination
scores = backward_feature_elimination(X_train, X_test, y_train, y_test, model)

# Plot the results
num_features, r2_scores = zip(*scores)
plt.figure(figsize=(10, 6))
plt.plot(num_features, r2_scores, marker='o')
plt.title('Backward Feature Elimination: R-squared vs Number of Features')
plt.xlabel('Number of Features')
plt.ylabel('R-squared Score')
plt.grid()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv("melbourne_housing_raw.csv")

# Handle missing values by dropping rows with missing values
df_cleaned = df.dropna()

# Convert categorical variables into numerical values using OneHotEncoder
df_encoded = pd.get_dummies(df_cleaned, drop_first=True)

# Define features (X) and target (y)
X = df_encoded.drop(columns=['Price'])
y = df_encoded['Price']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Get feature importances
importances = model.feature_importances_

# Create a DataFrame to hold feature names and their importances
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Display feature importances
print("\nFeature Importances:")
print(feature_importance_df)

# Plot feature importances
plt.figure(figsize=(12, 6))
plt.barh(feature_importance_df['Feature'][:10], feature_importance_df['Importance'][:10], color='blue')
plt.xlabel('Importance')
plt.title('Top 10 Feature Importances')
plt.gca().invert_yaxis()  # Invert y-axis for better visualization
plt.show()

# Function to remove least important features and assess model accuracy
def remove_least_important_features(X_train, X_test, y_train, y_test, model, feature_importance_df, threshold=0.01):
    # Keep features with importance above the threshold
    important_features = feature_importance_df[feature_importance_df['Importance'] > threshold]['Feature'].tolist()

    # Train and evaluate the model on selected features
    model.fit(X_train[important_features], y_train)
    y_pred_reduced = model.predict(X_test[important_features])

    mse_reduced = mean_squared_error(y_test, y_pred_reduced)
    r2_reduced = r2_score(y_test, y_pred_reduced)

    print(f"\nMean Squared Error after removing least important features: {mse_reduced}")
    print(f"R-squared after removing least important features: {r2_reduced}")
    return important_features

# Call the function to assess model accuracy after removing least important features
remaining_features = remove_least_important_features(X_train, X_test, y_train, y_test, model, feature_importance_df)
print("\nRemaining features after removal:", remaining_features)

